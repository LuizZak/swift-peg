@grammar """
start: a ;
a: c.b+< d | c.b+> d;
b[TokenType]: 'b' ;
c[TokenType]: 'c' ;
d[TokenType]: 'd' ;
""" ;

@expectedParser """
extension Parser {
    /// ```
    /// start:
    ///     | a
    ///     ;
    /// ```
    @memoized("start")
    @inlinable
    public func __start() throws -> Node? {
        let _mark = self.mark()

        if
            let a = try self.a()
        {
            return a
        }

        self.restore(_mark)
        return nil
    }

    /// ```
    /// a:
    ///     | c.b+< d
    ///     | c.b+> d
    ///     ;
    /// ```
    @memoized("a")
    @inlinable
    public func __a() throws -> Node? {
        let _mark = self.mark()

        if
            case let (b?, d?) = self.shuffleTuple(try self._a_nsr())
        {
            return Node()
        }

        self.restore(_mark)

        if
            case let (b?, d?) = self.shuffleTuple(try self._a_nsr1())
        {
            return Node()
        }

        self.restore(_mark)
        return nil
    }

    /// ```
    /// b[TokenType]:
    ///     | 'b'
    ///     ;
    /// ```
    @memoized("b")
    @inlinable
    public func __b() throws -> TokenType? {
        let _mark = self.mark()

        if
            let _ = try self.expect("b")
        {
            return TokenType()
        }

        self.restore(_mark)
        return nil
    }

    /// ```
    /// c[TokenType]:
    ///     | 'c'
    ///     ;
    /// ```
    @memoized("c")
    @inlinable
    public func __c() throws -> TokenType? {
        let _mark = self.mark()

        if
            let _ = try self.expect("c")
        {
            return TokenType()
        }

        self.restore(_mark)
        return nil
    }

    /// ```
    /// d[TokenType]:
    ///     | 'd'
    ///     ;
    /// ```
    @memoized("d")
    @inlinable
    public func __d() throws -> TokenType? {
        let _mark = self.mark()

        if
            let _ = try self.expect("d")
        {
            return TokenType()
        }

        self.restore(_mark)
        return nil
    }

    /// ```
    /// _a_nsr[([TokenType], TokenType)]:
    ///     | c.b+< d
    ///     ;
    /// ```
    @memoized("_a_nsr")
    @inlinable
    public func ___a_nsr() throws -> ([TokenType], TokenType)? {
        let _mark = self.mark()

        var _current: [TokenType] = []

        while
            let b = try self.b()
        {
            _current.append(b)
            let _mark1 = self.mark()

            if
                let d = try self.d()
            {
                return (_current, d)
            }

            self.restore(_mark1)

            // Try separator before next item
            guard
                let _ = try self.c()
            else {
                break
            }
        }

        self.restore(_mark)
        return nil
    }

    /// ```
    /// _a_nsr1[([TokenType], TokenType)]:
    ///     | c.b+> d
    ///     ;
    /// ```
    @memoized("_a_nsr1")
    @inlinable
    public func ___a_nsr1() throws -> ([TokenType], TokenType)? {
        let _mark = self.mark()

        // Start by fetching as many productions as possible
        guard
            var _current: [(Mark, TokenType)] = try self.gather(separator: {
                try self.c()
            }, item: {
                if let b = try self.b() { return (self.mark(), b) }
                return nil
            })
        else {
            return nil
        }

        while let _endMark = _current.last?.0 {
            self.restore(_endMark)

            if
                let d = try self.d()
            {
                return (_current.map(\\.1), d)
            } else if _current.count <= 1 {
                break
            }

            // Drop an item, backtrack the parser, and try again
            _current.removeLast()
        }

        self.restore(_mark)
        return nil
    }
}
""" ;

start: '' ;
